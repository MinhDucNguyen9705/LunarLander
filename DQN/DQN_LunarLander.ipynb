{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd834f1b-51c4-4910-a4d6-3b212e1a2a5a",
   "metadata": {
    "id": "fd834f1b-51c4-4910-a4d6-3b212e1a2a5a"
   },
   "source": [
    "## Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22518486-3c2f-47fe-92b3-1502875eacfe",
   "metadata": {
    "id": "22518486-3c2f-47fe-92b3-1502875eacfe"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import base64, io\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f934c-6921-43aa-8389-6df4b993eca4",
   "metadata": {
    "id": "f75f934c-6921-43aa-8389-6df4b993eca4"
   },
   "source": [
    "### Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "DzTePA9axeje",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzTePA9axeje",
    "outputId": "4fca5039-5374-49fd-c1e4-c120bd67ea2e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(r\"c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\")\n",
    "# !pip install gymnasium\n",
    "import gymnasium as gym\n",
    "# !pip install swig\n",
    "# !pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594828b6-da33-481d-ab42-041e8c17ffea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "594828b6-da33-481d-ab42-041e8c17ffea",
    "outputId": "56d5f5d1-eb48-4de9-c439-855b6bfd2d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v3')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03735bdc-c07e-4c87-b208-cce894bb8e43",
   "metadata": {
    "id": "03735bdc-c07e-4c87-b208-cce894bb8e43"
   },
   "source": [
    "### Define Neural Network Architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae834607-433e-4ed5-8b23-8de7b53230a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae834607-433e-4ed5-8b23-8de7b53230a8",
    "outputId": "3d77a814-4171-4dcc-d06f-ca443c147ac7"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state #cac thong tin ve state hien tai\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)\n",
    "    #output: a vector with Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0873298-cab0-4dcb-9c84-4782dc914dd7",
   "metadata": {
    "id": "b0873298-cab0-4dcb-9c84-4782dc914dd7"
   },
   "source": [
    "### Define some hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7010c525-29d8-445c-8769-6cfb7d00948b",
   "metadata": {
    "id": "7010c525-29d8-445c-8769-6cfb7d00948b"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334bb7d8-7d62-4cfb-96f7-f8809ba8e089",
   "metadata": {
    "id": "334bb7d8-7d62-4cfb-96f7-f8809ba8e089"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d861efe-200c-4690-9698-722abbf0b77c",
   "metadata": {
    "id": "1d861efe-200c-4690-9698-722abbf0b77c"
   },
   "source": [
    "### Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0530f456-2bfd-4061-ad62-f14846a9a284",
   "metadata": {
    "id": "0530f456-2bfd-4061-ad62-f14846a9a284"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)  #update after each step\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device) #update after few steps\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed) #tránh phụ thuộc quá nhiều vào các bước gần nhau.\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5db66-ecb3-4bf4-aee1-3e18003c17b0",
   "metadata": {
    "id": "4eb5db66-ecb3-4bf4-aee1-3e18003c17b0"
   },
   "source": [
    "### Define Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f74bb08e-0b95-42db-9fc8-d609514d55af",
   "metadata": {
    "id": "f74bb08e-0b95-42db-9fc8-d609514d55af"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd61b8d-b63e-444f-9cb7-6295df46995d",
   "metadata": {
    "id": "6bd61b8d-b63e-444f-9cb7-6295df46995d"
   },
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c907ab57-ed48-4824-b27c-8c7d707d6919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "c907ab57-ed48-4824-b27c-8c7d707d6919",
    "outputId": "2879ac94-7650-4bdd-ac0d-1323c1e4bf28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 62\tAverage Score: -176.88"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m     40\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(state_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, action_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mdqn\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     19\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[1;32m---> 21\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     23\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstep(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 50\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Epsilon-greedy action selection\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a network that maps state -> action values.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0])\n",
    "        state = state.reshape(1, -1)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=210.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6726a-977a-4345-8897-021302bfc262",
   "metadata": {
    "id": "1ba6726a-977a-4345-8897-021302bfc262"
   },
   "source": [
    "### Plot the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2d491c9-a5dc-4c32-a95d-796f85c60c83",
   "metadata": {
    "id": "d2d491c9-a5dc-4c32-a95d-796f85c60c83"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mscores\u001b[49m)), scores)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode #\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5KR6aEVcaM8I",
   "metadata": {
    "id": "5KR6aEVcaM8I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: -826.80\n"
     ]
    }
   ],
   "source": [
    "#Evaluate:\n",
    "def evaluate(n_episodes=10, max_t=1000):\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps=0.0)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "scores = evaluate()\n",
    "print(\"Average Score: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "i9HhuP6tLlAh",
   "metadata": {
    "id": "i9HhuP6tLlAh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10896\\3265657965.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.qnetwork_local.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 , reward 0.32051452851155315 , current reward:  0.32051452851155315\n",
      "Step:  2 , reward 0.35027390716015927 , current reward:  0.6707884356717124\n",
      "Step:  3 , reward 0.21314895462936193 , current reward:  0.8839373903010743\n",
      "Step:  4 , reward -0.6313111948772985 , current reward:  0.25262619542377585\n",
      "Step:  5 , reward -0.7063595712605206 , current reward:  -0.4537333758367448\n",
      "Step:  6 , reward 0.2514437734025432 , current reward:  -0.20228960243420158\n",
      "Step:  7 , reward -1.353444088842366 , current reward:  -1.5557336912765676\n",
      "Step:  8 , reward -1.393001899715813 , current reward:  -2.9487355909923805\n",
      "Step:  9 , reward -1.4220960484726959 , current reward:  -4.370831639465076\n",
      "Step:  10 , reward -1.4416521117933883 , current reward:  -5.812483751258465\n",
      "Step:  11 , reward -1.4527605947481845 , current reward:  -7.265244346006649\n",
      "Step:  12 , reward -1.4563521563788697 , current reward:  -8.72159650238552\n",
      "Step:  13 , reward -1.4532830127523368 , current reward:  -10.174879515137857\n",
      "Step:  14 , reward -1.444290465285576 , current reward:  -11.619169980423433\n",
      "Step:  15 , reward -1.4301121823777976 , current reward:  -13.04928216280123\n",
      "Step:  16 , reward -1.4113652878983203 , current reward:  -14.46064745069955\n",
      "Step:  17 , reward -1.388617125201165 , current reward:  -15.849264575900715\n",
      "Step:  18 , reward -1.3623488693607158 , current reward:  -17.21161344526143\n",
      "Step:  19 , reward -1.332977165562312 , current reward:  -18.544590610823743\n",
      "Step:  20 , reward -1.3008466860557064 , current reward:  -19.84543729687945\n",
      "Step:  21 , reward -1.2663629572887771 , current reward:  -21.111800254168227\n",
      "Step:  22 , reward -1.2297388480153586 , current reward:  -22.341539102183585\n",
      "Step:  23 , reward -1.1912722662784176 , current reward:  -23.532811368462003\n",
      "Step:  24 , reward -1.1511910430688204 , current reward:  -24.684002411530823\n",
      "Step:  25 , reward -1.197901052931968 , current reward:  -25.88190346446279\n",
      "Step:  26 , reward -1.0654352940956073 , current reward:  -26.9473387585584\n",
      "Step:  27 , reward 1.4726883421464094 , current reward:  -25.47465041641199\n",
      "Step:  28 , reward 4.9579208476227565 , current reward:  -20.516729568789234\n",
      "Step:  29 , reward 5.032021227153263 , current reward:  -15.484708341635972\n",
      "Step:  30 , reward -1.140624170058004 , current reward:  -16.625332511693976\n",
      "Step:  31 , reward 3.520144373576085 , current reward:  -13.105188138117892\n",
      "Step:  32 , reward 3.7738389188786643 , current reward:  -9.331349219239229\n",
      "Step:  33 , reward -1.34662336533097 , current reward:  -10.677972584570199\n",
      "Step:  34 , reward 1.3978300355150395 , current reward:  -9.28014254905516\n",
      "Step:  35 , reward 3.2383650964233768 , current reward:  -6.041777452631783\n",
      "Step:  36 , reward 4.438240378498858 , current reward:  -1.603537074132925\n",
      "Step:  37 , reward -1.324212481925258 , current reward:  -2.927749556058183\n",
      "Step:  38 , reward 1.9978400456594556 , current reward:  -0.9299095103987274\n",
      "Step:  39 , reward 4.198058811668983 , current reward:  3.268149301270256\n",
      "Step:  40 , reward -1.3855320992648217 , current reward:  1.8826172020054344\n",
      "Step:  41 , reward 2.2193409346620685 , current reward:  4.101958136667503\n",
      "Step:  42 , reward 4.322382427999526 , current reward:  8.424340564667029\n",
      "Step:  43 , reward -1.4469832916346377 , current reward:  6.977357273032391\n",
      "Step:  44 , reward 2.816452458131022 , current reward:  9.793809731163414\n",
      "Step:  45 , reward 3.3324258145700982 , current reward:  13.126235545733511\n",
      "Step:  46 , reward 1.7096181556752128 , current reward:  14.835853701408723\n",
      "Step:  47 , reward 1.8222679122201952 , current reward:  16.658121613628918\n",
      "Step:  48 , reward 4.91324067251399 , current reward:  21.571362286142907\n",
      "Step:  49 , reward -1.901158469845824 , current reward:  19.670203816297082\n",
      "Step:  50 , reward 2.488461486621401 , current reward:  22.158665302918482\n",
      "Step:  51 , reward 2.194452395791086 , current reward:  24.353117698709568\n",
      "Step:  52 , reward 4.405681740133997 , current reward:  28.758799438843564\n",
      "Step:  53 , reward -1.8919241497433734 , current reward:  26.86687528910019\n",
      "Step:  54 , reward -1.505196763337608 , current reward:  25.36167852576258\n",
      "Step:  55 , reward 1.6977743031125783 , current reward:  27.05945282887516\n",
      "Step:  56 , reward 1.778934141490322 , current reward:  28.83838697036548\n",
      "Step:  57 , reward 2.5017675730988005 , current reward:  31.34015454346428\n",
      "Step:  58 , reward 3.6329729712402754 , current reward:  34.97312751470456\n",
      "Step:  59 , reward 3.891426177067797 , current reward:  38.86455369177236\n",
      "Step:  60 , reward 3.646665180929898 , current reward:  42.51121887270226\n",
      "Step:  61 , reward 0.6106081197202229 , current reward:  43.121826992422484\n",
      "Step:  62 , reward 1.7233112702937887 , current reward:  44.845138262716276\n",
      "Step:  63 , reward 2.831690782154891 , current reward:  47.67682904487117\n",
      "Step:  64 , reward 2.9139041640255927 , current reward:  50.590733208896765\n",
      "Step:  65 , reward 4.357477224560125 , current reward:  54.94821043345689\n",
      "Step:  66 , reward 3.19303140968442 , current reward:  58.141241843141316\n",
      "Step:  67 , reward 1.746147973400906 , current reward:  59.887389816542225\n",
      "Step:  68 , reward 3.9618708230758672 , current reward:  63.849260639618095\n",
      "Step:  69 , reward -1.9050109983500778 , current reward:  61.944249641268016\n",
      "Step:  70 , reward 3.5218884485630015 , current reward:  65.46613808983102\n",
      "Step:  71 , reward 0.6618683540844443 , current reward:  66.12800644391547\n",
      "Step:  72 , reward 0.7092004768648053 , current reward:  66.83720692078028\n",
      "Step:  73 , reward 0.24638238622618475 , current reward:  67.08358930700646\n",
      "Step:  74 , reward 1.901274302357234 , current reward:  68.9848636093637\n",
      "Step:  75 , reward 2.4964639735725855 , current reward:  71.48132758293629\n",
      "Step:  76 , reward -1.6864616143635789 , current reward:  69.79486596857271\n",
      "Step:  77 , reward 3.1327506322115086 , current reward:  72.92761660078422\n",
      "Step:  78 , reward 0.5764863918467966 , current reward:  73.50410299263102\n",
      "Step:  79 , reward 1.2942982912271475 , current reward:  74.79840128385817\n",
      "Step:  80 , reward -1.7060116796259723 , current reward:  73.0923896042322\n",
      "Step:  81 , reward 4.5692715373468475 , current reward:  77.66166114157905\n",
      "Step:  82 , reward 2.1356755051482823 , current reward:  79.79733664672733\n",
      "Step:  83 , reward -1.5708748019834065 , current reward:  78.22646184474392\n",
      "Step:  84 , reward -2.044098394628391 , current reward:  76.18236345011553\n",
      "Step:  85 , reward 4.324347769281215 , current reward:  80.50671121939675\n",
      "Step:  86 , reward 2.5220024166172808 , current reward:  83.02871363601403\n",
      "Step:  87 , reward -1.6102189532787452 , current reward:  81.41849468273529\n",
      "Step:  88 , reward -2.2008672559215072 , current reward:  79.21762742681378\n",
      "Step:  89 , reward 3.411439640905738 , current reward:  82.62906706771952\n",
      "Step:  90 , reward 4.017002091556091 , current reward:  86.64606915927561\n",
      "Step:  91 , reward -1.5716463133740592 , current reward:  85.07442284590155\n",
      "Step:  92 , reward -2.340702177116809 , current reward:  82.73372066878474\n",
      "Step:  93 , reward 2.7029346727754104 , current reward:  85.43665534156015\n",
      "Step:  94 , reward -1.8434794482867858 , current reward:  83.59317589327337\n",
      "Step:  95 , reward 3.8758356663681157 , current reward:  87.46901155964149\n",
      "Step:  96 , reward -1.8837735269713534 , current reward:  85.58523803267013\n",
      "Step:  97 , reward -1.8792607853516898 , current reward:  83.70597724731844\n",
      "Step:  98 , reward 0.809382511564462 , current reward:  84.51535975888291\n",
      "Step:  99 , reward 1.4804138947861787 , current reward:  85.99577365366909\n",
      "Step:  100 , reward 1.7749810378787998 , current reward:  87.77075469154789\n",
      "Step:  101 , reward -1.7014405212785988 , current reward:  86.0693141702693\n",
      "Step:  102 , reward 3.3876630064202375 , current reward:  89.45697717668953\n",
      "Step:  103 , reward -1.5667888168227933 , current reward:  87.89018835986674\n",
      "Step:  104 , reward -1.6264371763539884 , current reward:  86.26375118351275\n",
      "Step:  105 , reward 2.496091265099662 , current reward:  88.75984244861242\n",
      "Step:  106 , reward -1.412148082501659 , current reward:  87.34769436611076\n",
      "Step:  107 , reward -1.4696388625147563 , current reward:  85.878055503596\n",
      "Step:  108 , reward 3.5835139162866456 , current reward:  89.46156941988265\n",
      "Step:  109 , reward -1.579503353906233 , current reward:  87.88206606597642\n",
      "Step:  110 , reward 1.5604866224856664 , current reward:  89.44255268846209\n",
      "Step:  111 , reward 2.9558049090247183 , current reward:  92.39835759748681\n",
      "Step:  112 , reward -1.760367651105966 , current reward:  90.63798994638084\n",
      "Step:  113 , reward 3.011043255580998 , current reward:  93.64903320196184\n",
      "Step:  114 , reward -1.8616499955335968 , current reward:  91.78738320642825\n",
      "Step:  115 , reward 3.0655758582025046 , current reward:  94.85295906463075\n",
      "Step:  116 , reward -1.6849294287461447 , current reward:  93.16802963588461\n",
      "Step:  117 , reward -1.7267830998437006 , current reward:  91.44124653604091\n",
      "Step:  118 , reward 2.1050260743253917 , current reward:  93.5462726103663\n",
      "Step:  119 , reward 1.726926545014419 , current reward:  95.27319915538072\n",
      "Step:  120 , reward -1.7075086649664968 , current reward:  93.56569049041423\n",
      "Step:  121 , reward 1.1958746175148007 , current reward:  94.76156510792903\n",
      "Step:  122 , reward -1.6049581292111839 , current reward:  93.15660697871785\n",
      "Step:  123 , reward 0.31624080281046646 , current reward:  93.47284778152832\n",
      "Step:  124 , reward 0.6050510868708414 , current reward:  94.07789886839916\n",
      "Step:  125 , reward -0.07862521204606593 , current reward:  93.9992736563531\n",
      "Step:  126 , reward -0.08903900191818365 , current reward:  93.91023465443492\n",
      "Step:  127 , reward -1.8003420682726983 , current reward:  92.10989258616222\n",
      "Step:  128 , reward 0.5581168977551954 , current reward:  92.66800948391742\n",
      "Step:  129 , reward 0.2450191292022737 , current reward:  92.91302861311969\n",
      "Step:  130 , reward 0.5464173538619235 , current reward:  93.45944596698162\n",
      "Step:  131 , reward 1.9624799539873095 , current reward:  95.42192592096893\n",
      "Step:  132 , reward -1.982113770630022 , current reward:  93.43981215033891\n",
      "Step:  133 , reward 0.5847035357611474 , current reward:  94.02451568610006\n",
      "Step:  134 , reward -2.6049190256222503 , current reward:  91.41959666047781\n",
      "Step:  135 , reward -1.7493743041414263 , current reward:  89.67022235633638\n",
      "Step:  136 , reward 0.7086582042141287 , current reward:  90.37888056055051\n",
      "Step:  137 , reward 0.7164879147322665 , current reward:  91.09536847528278\n",
      "Step:  138 , reward 0.7493718277544132 , current reward:  91.8447403030372\n",
      "Step:  139 , reward -2.1393095403932265 , current reward:  89.70543076264397\n",
      "Step:  140 , reward 2.4335883579249726 , current reward:  92.13901912056895\n",
      "Step:  141 , reward -0.7255102885250608 , current reward:  91.41350883204389\n",
      "Step:  142 , reward 1.1513265799439523 , current reward:  92.56483541198784\n",
      "Step:  143 , reward 0.6174952046959248 , current reward:  93.18233061668377\n",
      "Step:  144 , reward 0.912752813762485 , current reward:  94.09508343044625\n",
      "Step:  145 , reward 2.130796587496964 , current reward:  96.22588001794321\n",
      "Step:  146 , reward -2.031839974410565 , current reward:  94.19404004353265\n",
      "Step:  147 , reward -1.378861489871305 , current reward:  92.81517855366135\n",
      "Step:  148 , reward 2.083541861435717 , current reward:  94.89872041509707\n",
      "Step:  149 , reward 3.4948114808926904 , current reward:  98.39353189598977\n",
      "Step:  150 , reward 1.3034905420565124 , current reward:  99.69702243804628\n",
      "Step:  151 , reward -1.3296389892421487 , current reward:  98.36738344880413\n",
      "Step:  152 , reward 2.392309066412598 , current reward:  100.75969251521673\n",
      "Step:  153 , reward -1.3094114858281003 , current reward:  99.45028102938863\n",
      "Step:  154 , reward -1.454520369616688 , current reward:  97.99576065977195\n",
      "Step:  155 , reward 3.486437018500678 , current reward:  101.48219767827263\n",
      "Step:  156 , reward 2.103025467980925 , current reward:  103.58522314625355\n",
      "Step:  157 , reward -1.3523645766306132 , current reward:  102.23285856962295\n",
      "Step:  158 , reward 4.102817208777293 , current reward:  106.33567577840024\n",
      "Step:  159 , reward -1.3070894272025981 , current reward:  105.02858635119765\n",
      "Step:  160 , reward -1.4873311498894495 , current reward:  103.54125520130819\n",
      "Step:  161 , reward 1.2157071932354213 , current reward:  104.7569623945436\n",
      "Step:  162 , reward -1.5052746631375342 , current reward:  103.25168773140607\n",
      "Step:  163 , reward 0.5573618778041023 , current reward:  103.80904960921016\n",
      "Step:  164 , reward 1.9442553650698258 , current reward:  105.75330497427998\n",
      "Step:  165 , reward -1.407731681082808 , current reward:  104.34557329319718\n",
      "Step:  166 , reward -1.5391560103982158 , current reward:  102.80641728279896\n",
      "Step:  167 , reward 0.5571611034036195 , current reward:  103.36357838620258\n",
      "Step:  168 , reward 1.2091425850530768 , current reward:  104.57272097125565\n",
      "Step:  169 , reward -1.751850168755844 , current reward:  102.8208708024998\n",
      "Step:  170 , reward 0.28961728818196236 , current reward:  103.11048809068176\n",
      "Step:  171 , reward 1.7063574652569542 , current reward:  104.81684555593871\n",
      "Step:  172 , reward 2.041904441094876 , current reward:  106.85874999703358\n",
      "Step:  173 , reward -1.8450042595661031 , current reward:  105.01374573746747\n",
      "Step:  174 , reward 0.10977637158800918 , current reward:  105.12352210905549\n",
      "Step:  175 , reward -1.7571618976292172 , current reward:  103.36636021142627\n",
      "Step:  176 , reward 0.1497719755541283 , current reward:  103.5161321869804\n",
      "Step:  177 , reward -1.7794988548175468 , current reward:  101.73663333216285\n",
      "Step:  178 , reward -0.3503576684871092 , current reward:  101.38627566367575\n",
      "Step:  179 , reward 1.5884569777972046 , current reward:  102.97473264147295\n",
      "Step:  180 , reward -1.6877487081236708 , current reward:  101.28698393334929\n",
      "Step:  181 , reward 2.4367531525431945 , current reward:  103.72373708589248\n",
      "Step:  182 , reward 1.184385415914609 , current reward:  104.90812250180709\n",
      "Step:  183 , reward -1.8095120509585954 , current reward:  103.0986104508485\n",
      "Step:  184 , reward -0.12471514539288647 , current reward:  102.97389530545561\n",
      "Step:  185 , reward -0.8077288602492831 , current reward:  102.16616644520633\n",
      "Step:  186 , reward -0.26086924780262705 , current reward:  101.9052971974037\n",
      "Step:  187 , reward 2.8410848434067946 , current reward:  104.74638204081049\n",
      "Step:  188 , reward 3.0208032595768772 , current reward:  107.76718530038737\n",
      "Step:  189 , reward -1.6505154449360617 , current reward:  106.1166698554513\n",
      "Step:  190 , reward 0.974935033381567 , current reward:  107.09160488883288\n",
      "Step:  191 , reward -1.5676529547899918 , current reward:  105.52395193404288\n",
      "Step:  192 , reward 2.0916917382278983 , current reward:  107.61564367227078\n",
      "Step:  193 , reward -1.6070708894052004 , current reward:  106.00857278286557\n",
      "Step:  194 , reward 1.994203000901966 , current reward:  108.00277578376753\n",
      "Step:  195 , reward -1.4344587637571706 , current reward:  106.56831702001037\n",
      "Step:  196 , reward -1.5550475901877263 , current reward:  105.01326942982264\n",
      "Step:  197 , reward 0.2451896156315712 , current reward:  105.25845904545422\n",
      "Step:  198 , reward -1.4120202352398508 , current reward:  103.84643881021437\n",
      "Step:  199 , reward 2.9432092070935996 , current reward:  106.78964801730797\n",
      "Step:  200 , reward -1.4330698130077906 , current reward:  105.35657820430018\n",
      "Step:  201 , reward 1.029670068748868 , current reward:  106.38624827304905\n",
      "Step:  202 , reward -1.3293801099587128 , current reward:  105.05686816309034\n",
      "Step:  203 , reward 2.297107909447594 , current reward:  107.35397607253793\n",
      "Step:  204 , reward -1.7595792226092186 , current reward:  105.59439684992871\n",
      "Step:  205 , reward 1.5794916842020086 , current reward:  107.17388853413073\n",
      "Step:  206 , reward 1.3185264654223487 , current reward:  108.49241499955308\n",
      "Step:  207 , reward 3.4168745226243233 , current reward:  111.9092895221774\n",
      "Step:  208 , reward -1.651514800328684 , current reward:  110.25777472184872\n",
      "Step:  209 , reward -0.191222602964028 , current reward:  110.06655211888469\n",
      "Step:  210 , reward -1.7219491970999812 , current reward:  108.34460292178471\n",
      "Step:  211 , reward 2.00115996126807 , current reward:  110.34576288305279\n",
      "Step:  212 , reward -1.6590784071514122 , current reward:  108.68668447590137\n",
      "Step:  213 , reward 2.244533837012537 , current reward:  110.93121831291391\n",
      "Step:  214 , reward 1.979070402674435 , current reward:  112.91028871558835\n",
      "Step:  215 , reward -1.630546650353864 , current reward:  111.27974206523449\n",
      "Step:  216 , reward 2.064763292436564 , current reward:  113.34450535767105\n",
      "Step:  217 , reward 1.5818991495555508 , current reward:  114.92640450722661\n",
      "Step:  218 , reward -1.5482425713862398 , current reward:  113.37816193584037\n",
      "Step:  219 , reward -1.7188423189697488 , current reward:  111.65931961687062\n",
      "Step:  220 , reward 1.5652891270635834 , current reward:  113.2246087439342\n",
      "Step:  221 , reward -1.6120017979035879 , current reward:  111.61260694603061\n",
      "Step:  222 , reward 1.2023280472076479 , current reward:  112.81493499323825\n",
      "Step:  223 , reward 2.2548582162183424 , current reward:  115.06979320945659\n",
      "Step:  224 , reward -1.6153412504943674 , current reward:  113.45445195896222\n",
      "Step:  225 , reward -1.1470378519443287 , current reward:  112.30741410701789\n",
      "Step:  226 , reward -1.725567757607017 , current reward:  110.58184634941088\n",
      "Step:  227 , reward 0.5518282207676826 , current reward:  111.13367457017857\n",
      "Step:  228 , reward 1.3650593962648514 , current reward:  112.49873396644342\n",
      "Step:  229 , reward 0.5069695160463119 , current reward:  113.00570348248974\n",
      "Step:  230 , reward -1.6365691332960992 , current reward:  111.36913434919364\n",
      "Step:  231 , reward -0.3777416250401046 , current reward:  110.99139272415354\n",
      "Step:  232 , reward -1.7548090326754249 , current reward:  109.23658369147812\n",
      "Step:  233 , reward 1.8988054590883727 , current reward:  111.1353891505665\n",
      "Step:  234 , reward -1.7468034664304533 , current reward:  109.38858568413605\n",
      "Step:  235 , reward 0.01906907447229217 , current reward:  109.40765475860834\n",
      "Step:  236 , reward 0.9085528089704062 , current reward:  110.31620756757876\n",
      "Step:  237 , reward 2.5924455795692127 , current reward:  112.90865314714797\n",
      "Step:  238 , reward 0.9960458241664056 , current reward:  113.90469897131437\n",
      "Step:  239 , reward 1.883036796378662 , current reward:  115.78773576769304\n",
      "Step:  240 , reward 2.729505626121971 , current reward:  118.51724139381501\n",
      "Step:  241 , reward -1.8066492960535605 , current reward:  116.71059209776145\n",
      "Step:  242 , reward 3.0593403647440036 , current reward:  119.76993246250545\n",
      "Step:  243 , reward -1.8216446078343047 , current reward:  117.94828785467115\n",
      "Step:  244 , reward 1.6163195221908502 , current reward:  119.564607376862\n",
      "Step:  245 , reward 0.19973277174383314 , current reward:  119.76434014860584\n",
      "Step:  246 , reward 0.21913001165202034 , current reward:  119.98347016025787\n",
      "Step:  247 , reward -0.65155336053176 , current reward:  119.3319167997261\n",
      "Step:  248 , reward -0.6868821821531228 , current reward:  118.64503461757297\n",
      "Step:  249 , reward 0.5947647938745468 , current reward:  119.23979941144752\n",
      "Step:  250 , reward -0.07188562437708795 , current reward:  119.16791378707043\n",
      "Step:  251 , reward 3.453399200701324 , current reward:  122.62131298777176\n",
      "Step:  252 , reward -1.8475488398007371 , current reward:  120.77376414797102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  253 , reward -2.0735944068332515 , current reward:  118.70016974113777\n",
      "Step:  254 , reward -0.3569522993889322 , current reward:  118.34321744174883\n",
      "Step:  255 , reward -0.46139640757504025 , current reward:  117.8818210341738\n",
      "Step:  256 , reward 3.7142255104679593 , current reward:  121.59604654464175\n",
      "Step:  257 , reward -1.9649417342940936 , current reward:  119.63110481034767\n",
      "Step:  258 , reward -0.8950227315863615 , current reward:  118.7360820787613\n",
      "Step:  259 , reward -0.3505000435847549 , current reward:  118.38558203517654\n",
      "Step:  260 , reward -1.9577731068773474 , current reward:  116.42780892829919\n",
      "Step:  261 , reward 2.7500526894393564 , current reward:  119.17786161773854\n",
      "Step:  262 , reward -1.860676862208825 , current reward:  117.31718475552972\n",
      "Step:  263 , reward 2.399709918337387 , current reward:  119.71689467386712\n",
      "Step:  264 , reward -1.857872732854041 , current reward:  117.85902194101308\n",
      "Step:  265 , reward -0.03888767598378279 , current reward:  117.8201342650293\n",
      "Step:  266 , reward -1.9114619168351048 , current reward:  115.90867234819419\n",
      "Step:  267 , reward 1.4925267289950603 , current reward:  117.40119907718925\n",
      "Step:  268 , reward 0.06553432603750126 , current reward:  117.46673340322675\n",
      "Step:  269 , reward -0.48956806728049 , current reward:  116.97716533594627\n",
      "Step:  270 , reward -0.5233700523844107 , current reward:  116.45379528356186\n",
      "Step:  271 , reward -2.0414792815063123 , current reward:  114.41231600205555\n",
      "Step:  272 , reward 2.926832009530796 , current reward:  117.33914801158635\n",
      "Step:  273 , reward 1.5264669961925186 , current reward:  118.86561500777887\n",
      "Step:  274 , reward -1.7346740917880936 , current reward:  117.13094091599078\n",
      "Step:  275 , reward -2.103458129029576 , current reward:  115.0274827869612\n",
      "Step:  276 , reward 0.1673788211029546 , current reward:  115.19486160806416\n",
      "Step:  277 , reward 1.2122664020253502 , current reward:  116.40712801008951\n",
      "Step:  278 , reward 0.19400089293455985 , current reward:  116.60112890302406\n",
      "Step:  279 , reward -1.7597884179670888 , current reward:  114.84134048505697\n",
      "Step:  280 , reward 10.539547464080492 , current reward:  125.38088794913747\n",
      "Step:  281 , reward 1.8425248926200275 , current reward:  127.22341284175751\n",
      "Step:  282 , reward -9.932355753859975 , current reward:  117.29105708789753\n",
      "Step:  283 , reward -0.7200125717337016 , current reward:  116.57104451616382\n",
      "Step:  284 , reward 14.016672205320072 , current reward:  130.5877167214839\n",
      "Step:  285 , reward -0.13089248690779406 , current reward:  130.45682423457612\n",
      "Step:  286 , reward 12.711970763567379 , current reward:  143.16879499814348\n",
      "Step:  287 , reward 1.5991898906665174 , current reward:  144.76798488881\n",
      "Step:  288 , reward 1.04201130263856 , current reward:  145.80999619144856\n",
      "Step:  289 , reward -0.3568634683156606 , current reward:  145.4531327231329\n",
      "Step:  290 , reward -1.1106967644851178 , current reward:  144.3424359586478\n",
      "Step:  291 , reward 1.6966148507826588 , current reward:  146.03905080943045\n",
      "Step:  292 , reward 0.7142112618408945 , current reward:  146.75326207127134\n",
      "Step:  293 , reward 0.5553531356822603 , current reward:  147.3086152069536\n",
      "Step:  294 , reward 0.5638380899224025 , current reward:  147.872453296876\n",
      "Step:  295 , reward 0.33190890498880776 , current reward:  148.2043622018648\n",
      "Step:  296 , reward -0.10871691253070637 , current reward:  148.0956452893341\n",
      "Step:  297 , reward 0.07590223430709386 , current reward:  148.1715475236412\n",
      "Step:  298 , reward 0.08519261999559902 , current reward:  148.25674014363682\n",
      "Step:  299 , reward -0.022051015823510767 , current reward:  148.2346891278133\n",
      "Step:  300 , reward 0.007837643025794705 , current reward:  148.2425267708391\n",
      "Step:  301 , reward 0.015141930972781381 , current reward:  148.2576687018119\n",
      "Step:  302 , reward -0.00020526543748289328 , current reward:  148.25746343637442\n",
      "Step:  303 , reward 0.00011050476073748428 , current reward:  148.25757394113515\n",
      "Step:  304 , reward 0.002483955601643828 , current reward:  148.26005789673678\n",
      "Step:  305 , reward 0.0007100380891920111 , current reward:  148.26076793482596\n",
      "Step:  306 , reward -0.00022501778588335242 , current reward:  148.26054291704008\n",
      "Step:  307 , reward 0.0003767126991434111 , current reward:  148.26091962973922\n",
      "Step:  308 , reward 0.0002636771443285113 , current reward:  148.26118330688354\n",
      "Step:  309 , reward -9.042959532479244e-05 , current reward:  148.2610928772882\n",
      "Step:  310 , reward 4.597831178543288e-05 , current reward:  148.2611388556\n",
      "Step:  311 , reward 5.609270422013424e-05 , current reward:  148.26119494830422\n",
      "Step:  312 , reward -9.929585612411529e-06 , current reward:  148.2611850187186\n",
      "Step:  313 , reward 3.2165444636689244e-06 , current reward:  148.26118823526306\n",
      "Step:  314 , reward 7.543259883391329e-06 , current reward:  148.26119577852296\n",
      "Step:  315 , reward 3.701433371361418e-06 , current reward:  148.26119947995633\n",
      "Step:  316 , reward -3.7578781544311823e-06 , current reward:  148.26119572207818\n",
      "Step:  317 , reward -1.8814567184222142e-06 , current reward:  148.26119384062147\n",
      "Step:  318 , reward -4.981088004285539e-07 , current reward:  148.26119334251268\n",
      "Step:  319 , reward 100 , current reward:  248.26119334251268\n",
      "Video saved as lunar_lander_solution_3.mp4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "# Ensure your Agent class and dependencies are properly imported\n",
    "\n",
    "# Define constants\n",
    "ENV_NAME = \"LunarLander-v3\"\n",
    "MODEL_PATH = \"checkpoint.pth\"  # Path to your saved model\n",
    "VIDEO_FILENAME = \"lunar_lander_solution_3.mp4\"\n",
    "EPISODES = 1  # Number of episodes to record\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size, seed=0)\n",
    "\n",
    "# Load the trained weights\n",
    "agent.qnetwork_local.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "frames = []  # Store video frames\n",
    "\n",
    "# Run the environment\n",
    "total=0\n",
    "frame_counter=0\n",
    "for _ in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, eps=0.0)  # Use the trained policy\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        # Capture the frame\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        total += reward\n",
    "        frame_counter += 1\n",
    "        print(\"Step: \", frame_counter, \", reward\", reward, \", current reward: \", total)\n",
    "\n",
    "# Save video using imageio\n",
    "imageio.mimsave(VIDEO_FILENAME, frames, fps=30)\n",
    "print(f\"Video saved as {VIDEO_FILENAME}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WTOMhHN-NWer",
   "metadata": {
    "id": "WTOMhHN-NWer"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
